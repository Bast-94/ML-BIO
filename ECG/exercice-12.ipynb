{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/basth94/ml-bio-exercice-12?scriptVersionId=144223851\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Exercice 12:","metadata":{"id":"0pP3htYzA2Gv"}},{"cell_type":"markdown","source":"## Création d'un CNN pour la classification des scaleogrammes","metadata":{}},{"cell_type":"markdown","source":"[lien Nowledgeable](https://nowledgeable.com/invitation/student/join-module/9c8ec467-686a-44cd-a2f2-85cf174a79ad) <br>\nA l'aide de la transformée en ondelettes continue, transformez des signaux ECG en scaleograms et puis utilisez un modèle CNN pour classifier des images.\n","metadata":{"id":"8WzN9jVCTGVh"}},{"cell_type":"code","source":"!pip install wfdb","metadata":{"id":"wWhXzRJehB_8","outputId":"f27663e3-0597-4dee-b058-02448976e8c8","execution":{"iopub.status.busy":"2023-09-25T19:30:06.042571Z","iopub.execute_input":"2023-09-25T19:30:06.043114Z","iopub.status.idle":"2023-09-25T19:30:22.237888Z","shell.execute_reply.started":"2023-09-25T19:30:06.043073Z","shell.execute_reply":"2023-09-25T19:30:22.236396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pandas as pd\npd.set_option('display.max_columns', None)\n\nimport wfdb\nimport glob\nimport os\nimport pywt\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom scipy import signal as scipy_signal\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import  Dense, Dropout, Conv2D, Input, MaxPooling2D,Flatten\n","metadata":{"id":"y1jrUS_RhFpM","execution":{"iopub.status.busy":"2023-09-25T19:39:14.804207Z","iopub.execute_input":"2023-09-25T19:39:14.804684Z","iopub.status.idle":"2023-09-25T19:39:15.193029Z","shell.execute_reply.started":"2023-09-25T19:39:14.804653Z","shell.execute_reply":"2023-09-25T19:39:15.191583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Afin de pouvoir récupérer les signaux issus des datasets de physionet nous réutiliserons la fonction `load_signal_using_wfdb`","metadata":{}},{"cell_type":"code","source":"def load_signal_using_wfdb(file, start, end, channel, pn_dir):\n    \"\"\"\n    file: name of record\n    start: int\n    end:int\n    channel: 0 or 1\n    \"\"\"\n    record = wfdb.rdrecord(file, sampfrom = start, sampto = end, channels=[channel], pn_dir=pn_dir)\n    data = record.p_signal.reshape(-1)\n    return data\n","metadata":{"id":"sCw4fb5bhHKt","execution":{"iopub.status.busy":"2023-09-25T19:30:33.140884Z","iopub.execute_input":"2023-09-25T19:30:33.142339Z","iopub.status.idle":"2023-09-25T19:30:33.150347Z","shell.execute_reply.started":"2023-09-25T19:30:33.1423Z","shell.execute_reply":"2023-09-25T19:30:33.148607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous allons étudier 3 Datasets:\n- `nsrdb` : Normal Sinus Rhythm Database, les rythmes cardiaques normaux.\n- `mitdb` : Les rythmes cardiaques pour les personnes faisant de l'arythmie.\n- `chfdb` : Les rythmes cardiaques pour les personnes ayant des défaillances cardiaques.","metadata":{}},{"cell_type":"markdown","source":"Le but va être de générer un modèle convolutif comme suit:\n- Récupération des records.\n- Récupération des fréquences.\n- Labelisation des données.\n- Extraction et synchronisation des signaux issus de chaque record.\n- Découpage des signaux.\n- Récupération de leur CWT.\n- Passage dans le modèle convolutif.","metadata":{}},{"cell_type":"code","source":"database_names = ['nsrdb','mitdb','chfdb']\nn_label = len(database_names)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T19:30:33.152609Z","iopub.execute_input":"2023-09-25T19:30:33.153164Z","iopub.status.idle":"2023-09-25T19:30:33.170687Z","shell.execute_reply.started":"2023-09-25T19:30:33.153121Z","shell.execute_reply":"2023-09-25T19:30:33.168423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"record_names = []\nlabels = []\n\nfor i,name in enumerate(database_names):\n    current_record_names = wfdb.get_record_list(name)\n    record_names += current_record_names\n    labels += [i for record_name in current_record_names]\nassert len(record_names) == len(labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T19:30:33.175491Z","iopub.execute_input":"2023-09-25T19:30:33.176015Z","iopub.status.idle":"2023-09-25T19:30:34.771912Z","shell.execute_reply.started":"2023-09-25T19:30:33.175939Z","shell.execute_reply":"2023-09-25T19:30:34.770379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous récupèrons les fréquences pour permettre une bonne synchronisation.","metadata":{}},{"cell_type":"code","source":"frequencies = []\nfor record_name,label in tqdm(zip(record_names,labels)):\n    header = wfdb.rdheader(record_name,database_names[label])\n    frequencies.append(header.fs)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T19:38:06.949213Z","iopub.execute_input":"2023-09-25T19:38:06.949681Z","iopub.status.idle":"2023-09-25T19:38:44.020468Z","shell.execute_reply.started":"2023-09-25T19:38:06.949648Z","shell.execute_reply":"2023-09-25T19:38:44.018798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def synchronize_signal(signal, sampling_rate, common_sampling_rate):\n    \n\n    num_samples = int((common_sampling_rate*len(signal))/sampling_rate)\n    signal_resampled = scipy_signal.resample(signal, num_samples)\n\n    return signal_resampled","metadata":{"execution":{"iopub.status.busy":"2023-09-25T19:47:58.769899Z","iopub.execute_input":"2023-09-25T19:47:58.770358Z","iopub.status.idle":"2023-09-25T19:47:58.77869Z","shell.execute_reply.started":"2023-09-25T19:47:58.770325Z","shell.execute_reply":"2023-09-25T19:47:58.776998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Passons à l'extraction de la donnée:","metadata":{}},{"cell_type":"code","source":"def generate_data(sample_size,scale_size,nb_samples_per_signal , labels, record_names,database_names, file_path = None, frequencies = [] ,common_sampling_rate = 128 ):\n    \n    scales = range(1, scale_size+1)\n    waveletname = 'morl'\n    signal_ds = []\n    \n    y_train = np.zeros(nb_samples_per_signal*len(labels))\n    signal_ds = []\n    \n    for i in tqdm(range(len(record_names))):\n        record_name,label,frequency = record_names[i],labels[i],frequencies[i]\n        pn_dir = database_names[label]\n        signal=(load_signal_using_wfdb(record_name, start=0, end=nb_samples_per_signal*frequency, channel = 0, pn_dir=pn_dir))\n        signal = synchronize_signal(signal,frequency,common_sampling_rate)\n        signal_ds += np.split(signal,nb_samples_per_signal)\n        y_train[i*nb_samples_per_signal:(i+1)*nb_samples_per_signal] = label\n\n    signal_ds = np.array(signal_ds)\n    \n    X_train = []\n    for signal in signal_ds:\n        coeff, freqs = pywt.cwt(signal, scales, waveletname, 1)\n        X_train.append(coeff)\n    X_train = np.array(X_train)\n    X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],X_train.shape[2],1))\n    X_tensor = tf.convert_to_tensor(X_train)\n\n    num_classes = len(np.unique(y_train))\n    y = tf.keras.utils.to_categorical(y_train,num_classes)\n    \n    y_tensor = tf.convert_to_tensor(y)\n    print(y_tensor[0])\n    return X_tensor,y_tensor\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:07:57.362728Z","iopub.execute_input":"2023-09-25T20:07:57.363267Z","iopub.status.idle":"2023-09-25T20:07:57.378744Z","shell.execute_reply.started":"2023-09-25T20:07:57.363231Z","shell.execute_reply":"2023-09-25T20:07:57.377057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Générons maintenant les données `X_tensor` et leurs label `y_tensor`","metadata":{}},{"cell_type":"code","source":"\nsample_size = 64\nscale_size = 64\nnb_samples_per_signal = 64\ncommon_sampling_rate = 64\n\nX_tensor, y_tensor = generate_data(sample_size = sample_size,scale_size = scale_size,nb_samples_per_signal = nb_samples_per_signal, labels= labels, record_names = record_names,database_names=database_names,file_path='signals_csv',frequencies = frequencies ,common_sampling_rate = common_sampling_rate)\nassert X_tensor.shape[0] == y_tensor.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:15:22.840999Z","iopub.execute_input":"2023-09-25T20:15:22.841458Z","iopub.status.idle":"2023-09-25T20:16:40.807699Z","shell.execute_reply.started":"2023-09-25T20:15:22.841426Z","shell.execute_reply":"2023-09-25T20:16:40.805998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(input_shape,num_class):\n    model = Sequential()\n    model.add(Input(shape=input_shape))\n    \n    model.add(Conv2D(filters=64,kernel_size=5, activation='relu'))\n    model.add(MaxPooling2D())\n    model.add(Dropout(0.5))\n    \n    model.add(Conv2D(filters=32,kernel_size=3, activation='relu'))\n    model.add(MaxPooling2D())\n    model.add(Dropout(0.5))\n    \n    model.add(Conv2D(filters=32,kernel_size=3, activation='relu'))\n    model.add(MaxPooling2D())\n\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(num_class,activation='softmax'))\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n    return model\nmodel = build_model(X_tensor.shape[1:],y_tensor.shape[1])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:17:58.786836Z","iopub.execute_input":"2023-09-25T20:17:58.787272Z","iopub.status.idle":"2023-09-25T20:17:58.958545Z","shell.execute_reply.started":"2023-09-25T20:17:58.787239Z","shell.execute_reply":"2023-09-25T20:17:58.957457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tensor.shape,y_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:17:09.795534Z","iopub.execute_input":"2023-09-25T20:17:09.796293Z","iopub.status.idle":"2023-09-25T20:17:09.805431Z","shell.execute_reply.started":"2023-09-25T20:17:09.796253Z","shell.execute_reply":"2023-09-25T20:17:09.803907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_tensor,y_tensor,validation_split=0.2,epochs=5)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:18:02.674053Z","iopub.execute_input":"2023-09-25T20:18:02.674504Z","iopub.status.idle":"2023-09-25T20:19:05.165021Z","shell.execute_reply.started":"2023-09-25T20:18:02.67447Z","shell.execute_reply":"2023-09-25T20:19:05.16316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous pouvons hélas remarquer que la validation accuracy ne s'élve pas là où l'accuracy de training converge vers 1, le modèle overfit il n'est soit pas adapté aux données soit les données ont été mal générées en terme de choix de dimensions où d'échantillonage.","metadata":{}}]}