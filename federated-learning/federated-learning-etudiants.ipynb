{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/basth94/federated-learning-etudiants?scriptVersionId=145010687\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117","metadata":{"id":"ALHpQCeQMamo","outputId":"6195fafe-4b7f-4457-a395-b02b810737d8","execution":{"iopub.status.busy":"2023-10-02T16:31:13.378329Z","iopub.execute_input":"2023-10-02T16:31:13.378751Z","iopub.status.idle":"2023-10-02T16:33:32.816647Z","shell.execute_reply.started":"2023-10-02T16:31:13.37872Z","shell.execute_reply":"2023-10-02T16:33:32.815538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install matplotlib\n!pip install numpy\n!pip install pandas\n!pip install scikit-learn","metadata":{"id":"6ld_3HmDr1z-","outputId":"0dedf726-725c-4c44-928c-ff611e44f22a","execution":{"iopub.status.busy":"2023-10-02T16:33:32.818643Z","iopub.execute_input":"2023-10-02T16:33:32.818967Z","iopub.status.idle":"2023-10-02T16:34:14.367012Z","shell.execute_reply.started":"2023-10-02T16:33:32.818936Z","shell.execute_reply":"2023-10-02T16:34:14.365462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## I. MNIST Data","metadata":{"id":"v-dxfgsjDNJV"}},{"cell_type":"markdown","source":"### Exercice 1 : Loading Data","metadata":{"id":"M7dz8HfgsPkT"}},{"cell_type":"code","source":"import torch\nimport random\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\n\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\n\nfrom copy import deepcopy\n\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"id":"b6P1hr56G3Dz","execution":{"iopub.status.busy":"2023-10-02T16:34:14.369509Z","iopub.execute_input":"2023-10-02T16:34:14.36996Z","iopub.status.idle":"2023-10-02T16:34:16.601013Z","shell.execute_reply.started":"2023-10-02T16:34:14.369923Z","shell.execute_reply":"2023-10-02T16:34:16.599605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1)Create a function iid_split. This function should take a dataset, nb_nodes, n_samples_per_node, batch_size, and shuffle as parameters. The goal is to divide the dataset into nb_nodes subsets (i.i.d.) and load each subset using PyTorch's DataLoader with the specified batch_size and shuffle, and then return a list of these DataLoaders.\n\nSteps:\nLoad Data: Use DataLoader to load n_samples_per_node from the dataset with shuffle.  \nSplit Data: Divide the loaded data into nb_nodes i.i.d subsets, create a DataLoader for each, and append it to a list.   \nReturn List: Return the list of DataLoaders created.   ","metadata":{"id":"Q2z8JQpFxJLe"}},{"cell_type":"code","source":"def iid_split(dataset, nb_nodes, n_samples_per_node, batch_size, shuffle):\n    data_loaders = []\n    for node in range(nb_nodes):\n        start_idx = node * n_samples_per_node\n        end_idx = start_idx + n_samples_per_node\n        subset = Subset(dataset, list(range(start_idx, end_idx)))\n        data_loader = DataLoader(subset, batch_size=batch_size, shuffle=shuffle)\n        data_loaders.append(data_loader)\n    \n    return data_loaders","metadata":{"id":"41pRKxCrxI4_","execution":{"iopub.status.busy":"2023-10-02T16:34:16.603492Z","iopub.execute_input":"2023-10-02T16:34:16.604026Z","iopub.status.idle":"2023-10-02T16:34:16.611301Z","shell.execute_reply.started":"2023-10-02T16:34:16.603994Z","shell.execute_reply":"2023-10-02T16:34:16.610252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2) Create a function non_iid_split, designed to divide a dataset non-i.i.d. It will receive parameters: dataset, nb_nodes, n_samples_per_node, batch_size, shuffle, and shuffle_digits.\n\nSteps:\nArrange Digits: Optionally shuffle digits and fairly split them among nb_nodes.  \nLoad Data: Utilize DataLoader to load nb_nodes*n_samples_per_node samples, considering the shuffle parameter.   \nSplit Data and Return: Create DataLoaders for each node containing samples with corresponding digits and append them to a list.   ","metadata":{"id":"rFssgNjnyDN2"}},{"cell_type":"code","source":"def non_iid_split(dataset, nb_nodes, n_samples_per_node, batch_size, shuffle, shuffle_digits=False):\n    assert(nb_nodes>0 and nb_nodes<=10)\n    digits=torch.arange(10) if shuffle_digits==False else torch.randperm(10, generator=torch.Generator().manual_seed(0))\n\n    # split the digits in a fair way\n    fairly_digits = torch.chunk(digits, nb_nodes)\n    loader = torch.utils.data.DataLoader(dataset,\n                                        batch_size=nb_nodes*n_samples_per_node,\n                                        shuffle=False)\n    \n    \n    images_train, labels_train = next(iter(loader))\n    dataloaders=list()\n    # print(fairly_digits)\n    for i in range(nb_nodes):\n        indexes = []\n        for index, label in enumerate(labels_train): \n            if(label.item() in fairly_digits[i]):\n                indexes.append(index)\n        subset = Subset(dataset, indexes)\n        \n        node_loader = DataLoader(subset, batch_size=batch_size, shuffle=shuffle)\n        \n        dataloaders.append(node_loader)\n        \n    return dataloaders","metadata":{"id":"Fj8_IBmRxcyX","outputId":"8f2e0ba6-e875-44d2-f55f-80fa8728d27b","execution":{"iopub.status.busy":"2023-10-02T17:51:49.522448Z","iopub.execute_input":"2023-10-02T17:51:49.523597Z","iopub.status.idle":"2023-10-02T17:51:49.537917Z","shell.execute_reply.started":"2023-10-02T17:51:49.523544Z","shell.execute_reply":"2023-10-02T17:51:49.536968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3) Create a function get_MNIST, that fetches the MNIST dataset and utilizes either iid_split or non_iid_split to return train and test DataLoaders. The parameters are type (\"iid\" or \"non_iid\"), n_samples_train, n_samples_test, n_clients, batch_size, and shuffle.\n\nSteps:\nLoad MNIST Dataset: Utilize the MNIST dataset from PyTorch datasets for both train and test.  \nApply Split Function: Depending on the type parameter, apply either iid_split or non_iid_split to the loaded datasets.   \nReturn DataLoaders: Return the created train and test DataLoaders lists.  \n","metadata":{"id":"GjL3PBSjzzpn"}},{"cell_type":"code","source":"def get_MNIST(type=\"iid\", n_samples_train=200, n_samples_test=100, n_clients=3, batch_size=25, shuffle=True):\n    \n    transform = transforms.Compose([transforms.ToTensor()])\n    train_dataset = MNIST('./mnist',download=True,transform=transform,train=True)\n    test_dataset = MNIST('./mnist',download=True,transform=transform,train=False)\n    train_loaders,test_loaders = [],[]\n    if(type==\"iid\"):\n        train_loaders = iid_split(train_dataset, n_clients, n_samples_train, batch_size, shuffle)\n        test_loaders = iid_split(test_dataset, n_clients, n_samples_test, batch_size, shuffle)\n    if(type==\"non_iid\"):\n        train_loaders = non_iid_split(train_dataset, n_clients, n_samples_train, batch_size, shuffle)\n        test_loaders = non_iid_split(test_dataset, n_clients, n_samples_test, batch_size, shuffle)\n    return train_loaders,test_loaders\n","metadata":{"id":"3Jcy4a_xzzQL","execution":{"iopub.status.busy":"2023-10-02T17:51:51.358025Z","iopub.execute_input":"2023-10-02T17:51:51.358473Z","iopub.status.idle":"2023-10-02T17:51:51.366585Z","shell.execute_reply.started":"2023-10-02T17:51:51.358439Z","shell.execute_reply":"2023-10-02T17:51:51.36499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4) Use get_MNIST to get mnist_iid_train and mnist_iid_test dataloaders.","metadata":{"id":"34Kn3WUR0xev"}},{"cell_type":"code","source":"mnist_iid_train,mnist_iid_test  = get_MNIST()","metadata":{"id":"uJi1e0KvJzuh","execution":{"iopub.status.busy":"2023-10-02T17:51:52.507442Z","iopub.execute_input":"2023-10-02T17:51:52.5078Z","iopub.status.idle":"2023-10-02T17:51:52.667217Z","shell.execute_reply.started":"2023-10-02T17:51:52.507773Z","shell.execute_reply":"2023-10-02T17:51:52.665982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5) Giving the following function plot_samples for iid data. Plot image samples of client 1, 2 and 3.","metadata":{"id":"6Ta793YC1Der"}},{"cell_type":"code","source":"def plot_samples(data, channel:int, title=None, plot_name=\"\", n_examples =20):\n\n    n_rows = int((n_examples-1) / 5) +1\n    fig = plt.figure(figsize=(1* n_rows, 1*n_rows))\n    if title:\n        plt.suptitle(title)\n    X, y= data\n    for idx in range(n_examples):\n\n        ax = fig.add_subplot(n_rows, 5, idx + 1)\n\n        image = 255 - X[idx, channel].view((28,28))\n        ax.imshow(image, cmap='gist_gray')\n        ax.axis(\"off\")\n\n    if plot_name!=\"\":\n        fig.savefig(plot_name+\".png\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"AqItEeb21LqZ","execution":{"iopub.status.busy":"2023-10-02T17:51:55.112147Z","iopub.execute_input":"2023-10-02T17:51:55.112986Z","iopub.status.idle":"2023-10-02T17:51:55.120023Z","shell.execute_reply.started":"2023-10-02T17:51:55.112952Z","shell.execute_reply":"2023-10-02T17:51:55.11905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,loader in enumerate(mnist_iid_train):\n    data = (next(iter(loader)))\n    plot_samples(data=data,channel=0,title=f'Client n°{i} samples',n_examples=10)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:51:55.299473Z","iopub.execute_input":"2023-10-02T17:51:55.300579Z","iopub.status.idle":"2023-10-02T17:51:56.229776Z","shell.execute_reply.started":"2023-10-02T17:51:55.300535Z","shell.execute_reply":"2023-10-02T17:51:56.228635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6) Plot samples this time for non_iid data for 3 clients.","metadata":{"id":"AnWOE24T3s3K"}},{"cell_type":"code","source":"mnist_non_iid_train, mnist_non_iid_test = get_MNIST(type=\"non_iid\",n_clients=3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,loader in enumerate(mnist_non_iid_train):\n    data = (next(iter(loader)))\n    \n    print(len(loader))\n    plot_samples(data=data,channel=0,title=f'Client n°{i} samples',n_examples=10)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:51:57.149283Z","iopub.execute_input":"2023-10-02T17:51:57.150067Z","iopub.status.idle":"2023-10-02T17:51:58.356197Z","shell.execute_reply.started":"2023-10-02T17:51:57.150023Z","shell.execute_reply":"2023-10-02T17:51:58.355447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exercice 2 : FedAvg and FedProx Implementation","metadata":{"id":"kMLHXjzz2YMk"}},{"cell_type":"markdown","source":"Here is a simple CNN.","metadata":{"id":"Awh8GJr52gf_"}},{"cell_type":"code","source":"class CNN(nn.Module):\n\n    \"\"\"ConvNet -> Max_Pool -> RELU -> ConvNet ->\n    Max_Pool -> RELU -> FC -> RELU -> FC -> SOFTMAX\"\"\"\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n\n        return x\n\nmodel_0 = CNN()","metadata":{"id":"8hl-nO2JGwiD","execution":{"iopub.status.busy":"2023-10-02T17:52:01.459186Z","iopub.execute_input":"2023-10-02T17:52:01.459698Z","iopub.status.idle":"2023-10-02T17:52:01.474988Z","shell.execute_reply.started":"2023-10-02T17:52:01.459663Z","shell.execute_reply":"2023-10-02T17:52:01.47373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1) Create a function difference_models_norm_2(model_1, model_2) which take two models, get models parameters and returns the sum of the square differences of models parameters.\n\n\n\n","metadata":{"id":"tqrpiwn14N3F"}},{"cell_type":"code","source":"def difference_models_norm_2(model_1, model_2):\n    params_1 = list(model_1.parameters())\n    params_2 = list(model_2.parameters())\n    sum_squared_diff = 0.0\n\n    \n    if len(params_1) != len(params_2):\n        raise ValueError(\"The models must have the same number of parameters.\")\n\n    for p1, p2 in zip(params_1, params_2):\n        diff = p1 - p2\n        sum_squared_diff += torch.sum(diff**2)\n\n    return sum_squared_diff","metadata":{"id":"Q-qTUlG85261","execution":{"iopub.status.busy":"2023-10-02T17:52:02.644012Z","iopub.execute_input":"2023-10-02T17:52:02.644488Z","iopub.status.idle":"2023-10-02T17:52:02.651725Z","shell.execute_reply.started":"2023-10-02T17:52:02.64445Z","shell.execute_reply":"2023-10-02T17:52:02.650241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is the function to perform one epoch of training data.","metadata":{"id":"UVAIDa8m5_Pz"}},{"cell_type":"code","source":"def train_step(model, model_0, mu:int, optimizer, train_data, loss_f):\n    \"\"\"Train `model` on one epoch of `train_data`\"\"\"\n\n    total_loss=0\n\n    for idx, (features,labels) in enumerate(train_data):\n\n        optimizer.zero_grad()\n\n        predictions= model(features)\n\n        loss=loss_f(predictions,labels)\n        loss+=mu/2*difference_models_norm_2(model,model_0)\n        total_loss+=loss\n\n        loss.backward()\n        optimizer.step()\n\n    return total_loss/(idx+1)","metadata":{"id":"jduyyFu36BrC","execution":{"iopub.status.busy":"2023-10-02T17:52:03.778534Z","iopub.execute_input":"2023-10-02T17:52:03.779638Z","iopub.status.idle":"2023-10-02T17:52:03.787922Z","shell.execute_reply.started":"2023-10-02T17:52:03.77959Z","shell.execute_reply":"2023-10-02T17:52:03.786343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2) Create a function local_learning which perform a local training of a model sent. We need to precise number of epochs needed to be performed locally. Use previous function defined.\n\nReturn the local_loss compute during local training.","metadata":{"id":"VYNLNhJI6ZXG"}},{"cell_type":"code","source":"\ndef local_learning(model, mu:float, optimizer, train_data, epochs:int, loss_f):\n\n    ### Copy model to a new variable ###\n    model_cpy = deepcopy(model)\n\n    for epoch in range(epochs):\n        local_loss=train_step(model,model_cpy,mu,optimizer,train_data, loss_f)\n\n    return float(local_loss.detach().numpy())","metadata":{"id":"mLNceMNa6bFL","execution":{"iopub.status.busy":"2023-10-02T17:52:05.276062Z","iopub.execute_input":"2023-10-02T17:52:05.276476Z","iopub.status.idle":"2023-10-02T17:52:05.284024Z","shell.execute_reply.started":"2023-10-02T17:52:05.276444Z","shell.execute_reply":"2023-10-02T17:52:05.28264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We defined 4 others usefull functions :\n- loss_classifier :\n- loss_dataset :\n- accuracy_dataset :\n- set_to_zero_model_weights :\n\nTake time to read it.","metadata":{"id":"yf89aLlG7n8L"}},{"cell_type":"code","source":"def loss_classifier(predictions,labels):\n\n    m = nn.LogSoftmax(dim=1)\n    loss = nn.NLLLoss(reduction=\"mean\")\n\n    return loss(m(predictions) ,labels.view(-1))\n\n\ndef loss_dataset(model, dataset, loss_f):\n    \"\"\"Compute the loss of `model` on `dataset`\"\"\"\n    loss=0\n\n    for idx,(features,labels) in enumerate(dataset):\n\n        predictions= model(features)\n        loss+=loss_f(predictions,labels)\n\n    loss/=idx+1\n    return loss\n\n\ndef accuracy_dataset(model, dataset):\n    \"\"\"Compute the accuracy of `model` on `dataset`\"\"\"\n\n    correct=0\n\n    for features,labels in iter(dataset):\n\n        predictions= model(features)\n\n        _,predicted=predictions.max(1,keepdim=True)\n\n        correct+=torch.sum(predicted.view(-1,1)==labels.view(-1, 1)).item()\n\n    accuracy = 100*correct/len(dataset.dataset)\n\n    return accuracy\n\ndef set_to_zero_model_weights(model):\n    \"\"\"Set all the parameters of a model to 0\"\"\"\n\n    for layer_weigths in model.parameters():\n        layer_weigths.data.sub_(layer_weigths.data)","metadata":{"id":"68VGErzD7-7-","execution":{"iopub.status.busy":"2023-10-02T17:52:06.426114Z","iopub.execute_input":"2023-10-02T17:52:06.426548Z","iopub.status.idle":"2023-10-02T17:52:06.435101Z","shell.execute_reply.started":"2023-10-02T17:52:06.426517Z","shell.execute_reply":"2023-10-02T17:52:06.434361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3) Defined a function average_models, which as it is written average parameters of a list of models to create a new model.\n","metadata":{"id":"qy4MEJU08K05"}},{"cell_type":"code","source":"def average_models(model, clients_models_hist:list , weights:list):\n\n\n    \"\"\"Creates the new model of a given iteration with the models of the other\n    clients\"\"\"\n    # print(clients_models_hist)\n    new_model=deepcopy(model)\n    set_to_zero_model_weights(new_model)\n\n    for k,client_hist in enumerate(clients_models_hist):\n\n        for idx, layer_weights in enumerate(new_model.parameters()):\n            contribution=client_hist[idx].data*weights[k]\n            layer_weights.data.add_(contribution)\n    \n    return new_model","metadata":{"id":"zUT09s28HCZm","execution":{"iopub.status.busy":"2023-10-02T18:20:06.647037Z","iopub.execute_input":"2023-10-02T18:20:06.647555Z","iopub.status.idle":"2023-10-02T18:20:06.654332Z","shell.execute_reply.started":"2023-10-02T18:20:06.647521Z","shell.execute_reply":"2023-10-02T18:20:06.653328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4) We will now code a function to perform FedProx.","metadata":{"id":"7hIGhpAL9aWu"}},{"cell_type":"code","source":"def FedProx(model, training_sets:list, n_iter:int, testing_sets:list, mu=0,\n    file_name=\"test\", epochs=5, lr=10**-2, decay=1):\n    \"\"\" all the clients are considered in this implementation of FedProx\n    Parameters:\n        - `model`: common structure used by the clients and the server\n        - `training_sets`: list of the training sets. At each index is the\n            training set of client \"index\"\n        - `n_iter`: number of iterations the server will run\n        - `testing_set`: list of the testing sets. If [], then the testing\n            accuracy is not computed\n        - `mu`: regularization term for FedProx. mu=0 for FedAvg\n        - `epochs`: number of epochs each client is running\n        - `lr`: learning rate of the optimizer\n        - `decay`: to change the learning rate at each iteration\n\n    returns :\n        - `model`: the final global model\n    \"\"\"\n\n    loss_f=loss_classifier\n\n    #Variables initialization\n    K=len(training_sets) #number of clients\n    n_samples=sum([len(db.dataset) for db in training_sets])\n    weights=([len(db.dataset)/n_samples for db in training_sets])\n    print(\"Clients' weights:\",weights)\n\n\n    loss_hist=[[float(loss_dataset(model, dl, loss_f).detach())\n        for dl in training_sets]]\n    acc_hist=[[accuracy_dataset(model, dl) for dl in testing_sets]]\n    server_hist=[[tens_param.detach().numpy()\n        for tens_param in list(model.parameters())]]\n    models_hist = []\n\n\n    server_loss=sum([weights[i]*loss_hist[-1][i] for i in range(len(weights))])\n    server_acc=sum([weights[i]*acc_hist[-1][i] for i in range(len(weights))])\n    print(f'====> i: 0 Loss: {server_loss} Server Test Accuracy: {server_acc}')\n\n    for i in range(n_iter):\n\n        clients_params=[]\n        clients_models=[]\n        clients_losses=[]\n\n        for k in range(K):\n\n            local_model=deepcopy(model)#torch.load(filename)\n            # Define optimizer for local_model, don't forget the learning rate !\n            local_optimizer=optim.SGD(local_model.parameters(), lr=lr)\n            # compute local_loss by performing learning steps on the current model\n            local_loss= local_learning(model=local_model, mu=mu, optimizer=local_optimizer, train_data=training_sets[k], epochs=epochs, loss_f=loss_f)\n\n            clients_losses.append(local_loss)\n\n            #GET THE PARAMETER TENSORS OF THE MODEL\n            list_params=list(local_model.parameters())\n            list_params=[tens_param.detach() for tens_param in list_params]\n            clients_params.append(list_params)\n            clients_models.append(deepcopy(local_model))\n\n\n        #CREATE THE NEW GLOBAL MODEL\n        # Create new global model by avering all locals models\n        model = average_models(model,clients_params,weights)\n        models_hist.append(clients_params)\n\n        #COMPUTE THE LOSS/ACCURACY OF THE DIFFERENT CLIENTS WITH THE NEW MODEL\n        loss_hist+=[[float(loss_dataset(model, dl, loss_f).detach())\n            for dl in training_sets]]\n        acc_hist+=[[accuracy_dataset(model, dl) for dl in testing_sets]]\n\n        server_loss=sum([weights[i]*loss_hist[-1][i] for i in range(len(weights))])\n        server_acc=sum([weights[i]*acc_hist[-1][i] for i in range(len(weights))])\n\n        print(f'====> i: {i+1} Loss: {server_loss} Server Test Accuracy: {server_acc}')\n\n\n        server_hist.append([tens_param.detach().cpu().numpy()\n            for tens_param in list(model.parameters())])\n\n        #DECREASING THE LEARNING RATE AT EACH SERVER ITERATION\n        lr*=decay\n\n    return model, loss_hist, acc_hist","metadata":{"id":"kxSLDqvyHHat","execution":{"iopub.status.busy":"2023-10-02T18:19:20.874746Z","iopub.execute_input":"2023-10-02T18:19:20.875381Z","iopub.status.idle":"2023-10-02T18:19:20.893781Z","shell.execute_reply.started":"2023-10-02T18:19:20.87534Z","shell.execute_reply":"2023-10-02T18:19:20.892003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Exercice 3 : iid data","metadata":{"id":"1NJ0VrGOHLMh"}},{"cell_type":"markdown","source":"#### Fed training with FedAvg","metadata":{"id":"tYVeXiobBvRV"}},{"cell_type":"markdown","source":"1) With FedProx compute a FedAvg on iid data.","metadata":{"id":"qURGwRNXAzjQ"}},{"cell_type":"code","source":"fpx_model, loss_hist, acc_hist = FedProx(model_0,mnist_iid_train,50,mnist_iid_test)","metadata":{"id":"rcuFJya6HP7U","execution":{"iopub.status.busy":"2023-10-02T18:42:43.044633Z","iopub.execute_input":"2023-10-02T18:42:43.045066Z","iopub.status.idle":"2023-10-02T18:45:02.404572Z","shell.execute_reply.started":"2023-10-02T18:42:43.045036Z","shell.execute_reply":"2023-10-02T18:45:02.403043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2) Define a function plot_accuracy_loss to plot accuracy and loss of performed FedAvg for the 3 different clients.","metadata":{"id":"PxECdf8cBItv"}},{"cell_type":"code","source":"def plot_acc_loss(title:str, loss_hist:list, acc_hist:list):\n    n_ax = 2\n    fig_ratio = 10\n    loss_hist_array = np.array(loss_hist)\n    acc_hist_array = np.array(acc_hist)\n    fig = plt.figure(figsize=(fig_ratio*n_ax,fig_ratio))\n    ax1 = fig.add_subplot(n_ax,1,1)\n    ax1.set_title(title)\n    ax2 = ax1.twinx()\n    for i in range(loss_hist_array.shape[1]):\n        ax1.plot(loss_hist_array[:,i],label=f'client {i+1} loss')\n        ax2.plot(acc_hist_array[:,i],label=f'client {i+1} accuracy')\n        \n    ax1.set_xlabel('iteration')\n    ax1.set_ylabel('Loss evolution')\n    \n    \n    ax2.set_ylabel('Accuracy')\n    ax1.set_ylabel('Loss')\n    \n    ax1.legend()\n    ax2.legend()\n    plt.tight_layout()\n    plt.show()\n    \nplot_acc_loss('Accuracy and loss with federative learning',loss_hist,acc_hist )","metadata":{"id":"gr56Yu3rBhny","execution":{"iopub.status.busy":"2023-10-02T18:47:12.651752Z","iopub.execute_input":"2023-10-02T18:47:12.653564Z","iopub.status.idle":"2023-10-02T18:47:13.149491Z","shell.execute_reply.started":"2023-10-02T18:47:12.653511Z","shell.execute_reply":"2023-10-02T18:47:13.148304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### FedPRox","metadata":{"id":"lYf1MbP2HlWD"}},{"cell_type":"markdown","source":"3) Do the same as for 1) and 2).\nYou can take 2 local epochs, a learning rate of 0.1, mu = 0.3 and 10 iterations.","metadata":{"id":"-6kik1CoB9C1"}},{"cell_type":"code","source":"fpx_model, loss_hist, acc_hist = FedProx(model_0,mnist_iid_train,10,mnist_iid_test,mu=0.3,epochs=2,lr=0.1)","metadata":{"id":"l8qBIDCZHnHB","execution":{"iopub.status.busy":"2023-10-02T18:54:36.853466Z","iopub.execute_input":"2023-10-02T18:54:36.854006Z","iopub.status.idle":"2023-10-02T18:54:49.933386Z","shell.execute_reply.started":"2023-10-02T18:54:36.853968Z","shell.execute_reply":"2023-10-02T18:54:49.932117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acc_loss('Accuracy and loss with federative learning on iid data',loss_hist,acc_hist )","metadata":{"id":"qy2a8uMWHt0u","execution":{"iopub.status.busy":"2023-10-02T18:56:50.775639Z","iopub.execute_input":"2023-10-02T18:56:50.776134Z","iopub.status.idle":"2023-10-02T18:56:51.45396Z","shell.execute_reply.started":"2023-10-02T18:56:50.776102Z","shell.execute_reply":"2023-10-02T18:56:51.451751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4) What do you observe ?","metadata":{"id":"DAxLyz7yKU47"}},{"cell_type":"markdown","source":"We can observe that all the client models are converging in the same way either on accuracy or loss.","metadata":{"id":"PbClZcITMbkk"}},{"cell_type":"markdown","source":"### Exercice 4 : Non iid data","metadata":{"id":"LH0AzCoOH3Lh"}},{"cell_type":"markdown","source":"1) Perform a FedAvg on data and plot accuracy and loss\n2) Do the same for a FedProx.","metadata":{"id":"J50kEMPUCLXl"}},{"cell_type":"code","source":"fpx_model, loss_hist, acc_hist = FedProx(model_0,mnist_non_iid_train,50,mnist_non_iid_test)","metadata":{"id":"jd9nHSdjH8S2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"60KikAGDH-m0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"RYtw0QuYIUEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"z9aQF5oqIYyx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"u5eBgOX5IjxG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"P6JHGfbiIq47"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2) What do you observe ?","metadata":{"id":"vnm-Y1ssKYHP"}},{"cell_type":"markdown","source":"","metadata":{"id":"YbokIHOLMdJS"}},{"cell_type":"markdown","source":"## II. Medical Data","metadata":{"id":"HvSvunf4ybED"}},{"cell_type":"markdown","source":"### Exercice 5 : medmnist","metadata":{"id":"7leu2_UgDa6G"}},{"cell_type":"markdown","source":"For medical datasets we will use medmnist package.  \nMedmnist is large-scale MNIST-like collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D : https://medmnist.com/","metadata":{"id":"M1B9G05NyCcG"}},{"cell_type":"code","source":"! pip install medmnist\n","metadata":{"id":"Xh3lsOPIaTFG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import medmnist","metadata":{"id":"Ha2lt3iFyfPD"}},{"cell_type":"code","source":"import medmnist","metadata":{"id":"MXoi95WManMi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can list all available datasets with this command","metadata":{"id":"M-CLcn6Gyj5r"}},{"cell_type":"code","source":"!python -m medmnist available","metadata":{"id":"IU9V12wxau2U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use BloodMNIST for the following exercices. BloodMnist is a dataset with 8 categories of blood cells capture by microscope.","metadata":{"id":"uJJAUxpZyq9J"}},{"cell_type":"markdown","source":"1. Import from medmnsit BloodMNIST","metadata":{"id":"GuzPzm2kzSmU"}},{"cell_type":"code","source":"from medmnist import BloodMNIST","metadata":{"id":"moFXevwia_GJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\n\nimport medmnist\nfrom medmnist import INFO, Evaluator","metadata":{"id":"IT2KI08ddjRS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Create a preprocessing pipeline for data, transforms it into Tensor and then normalize it with a mean of 0.5 and standard deviation of 0.5 also.","metadata":{"id":"H-Gay0IKzlFb"}},{"cell_type":"code","source":"","metadata":{"id":"SlOKfzEbzjuk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Create a train dataset and test dataset from data and apply previous preprocessing pipeline. You can use DataClass from pytorch to create datasets.","metadata":{"id":"-kBeKkaGz_8Q"}},{"cell_type":"code","source":"","metadata":{"id":"ILobNOH5dynG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Just to understand data print the 2 datasets to see differences with previous MNIST data.","metadata":{"id":"nkWOFiCQ04XJ"}},{"cell_type":"code","source":"","metadata":{"id":"-cIYr2MzshnZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. Create a data loader for train data and test dat, you can use a batch size of 128 and shuffle data.","metadata":{"id":"wRJ1ndDJKole"}},{"cell_type":"code","source":"","metadata":{"id":"eVfI7YIZ04H9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6. Modify previous functions non_iid_split, iid_split, get_MNIST and plot_samples to work with new data.  \na. non_iid_split : this time there are only 8 classes   \nb. iid_split : nothing really changed  \nc. get_MNIST : Like in 3. use train and test datasets with the preprocessing pipeline we defined previously.  \nd. plot_samples : Make it work for this new images. Also try to print to which classes belong every images, to see later if your iid_split and non_iid_split work well.","metadata":{"id":"i33g8uOl1V8Q"}},{"cell_type":"code","source":"","metadata":{"id":"48gVTHQyC7cs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"5VdEwGiCLEmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"k6CVY7NqLFic"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exercice 6 : Use federated learning on medical data","metadata":{"id":"0PQACmFlMiUL"}},{"cell_type":"markdown","source":"### IID data","metadata":{"id":"BALUn7umik0t"}},{"cell_type":"markdown","source":"1. Load train data and test data for 3 clients into IID datasets.","metadata":{"id":"A9g9Edac1pHO"}},{"cell_type":"code","source":"","metadata":{"id":"loSiYTVfC7cs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. plost samples with corresponding function.","metadata":{"id":"ynnjgo0R26h9"}},{"cell_type":"code","source":"","metadata":{"id":"KzWBwDtOC7ct"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Design a convlution neural network to perform classification.  \nYou can try to adpat previous convolution network.   \nOr also you can try this type of architecture :  \n- A first 2D convolution layer with an output_channels of 16, a BatchNormalisation and a Relu activation   \n- A second 2D conv layer identical but with a MaxPooling layer after  \n- A 3rd conv layer with an output_channels of 64, a BatchNormalisation and a Relu activation again.\n- A 4rd layer identical to 3rd one.  \n- A 5rd layer identical but with a MaxPooling added. Don't forget to use padding to not loose informations from corners.\n- Finally add dense layers to perform classification. You can add regularization layers between dropout for example.\n\n","metadata":{"id":"BLovf_tp4BPL"}},{"cell_type":"code","source":"","metadata":{"id":"n2iey_qPC7ct"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"xj7wDhAT1_-r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fed training with FedAvg","metadata":{"id":"9zxI4gdAC7cu"}},{"cell_type":"markdown","source":"4. Perform a FedAvg with same parameters as for MNIST datasets. Use 3 clients, etc. Try to run more iterations.","metadata":{"id":"Ou50J8N5Le5W"}},{"cell_type":"code","source":"","metadata":{"id":"fMnCTdoPC7cu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"25rt8KmMC7cv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### FedPRox","metadata":{"id":"PRzBpTLwC7cv"}},{"cell_type":"markdown","source":"5. As FedAvg perform a FedProx on iid data. Keep the same µ.","metadata":{"id":"Pt2Nkqt2Lve2"}},{"cell_type":"code","source":"","metadata":{"id":"qjgZCAhGC7cv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"0U9Wx0YHC7cv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MNIST Non iid","metadata":{"id":"T7Qhr8c8C7cv"}},{"cell_type":"markdown","source":"6. Do the same process (FedAVG, FedProx, plot loss and accuracy) but this time on non iid data.","metadata":{"id":"DzZ8qJYEL49j"}},{"cell_type":"code","source":"","metadata":{"id":"CdcJ9Z5SC7cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"sUizEj6SC7cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8bfPfHDgC7cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"RuzECkMDC7cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"TReVWSFpC7cx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"9Jg-OicjC7cx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7. Is results satisfying ? Submit some way to improve results and resolve possible unstabilities. For this you can check original paper : https://arxiv.org/pdf/1812.06127.pdf","metadata":{"id":"IvQ-4vYNq3pv"}},{"cell_type":"markdown","source":"","metadata":{"id":"pq6OAZjBLXb6"}},{"cell_type":"code","source":"","metadata":{"id":"tnw4bnmYrdjT"},"execution_count":null,"outputs":[]}]}